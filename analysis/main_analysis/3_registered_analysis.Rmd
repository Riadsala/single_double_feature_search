---
title: "Supplementary Materials: Pilot Analysis"
author: "ADF Clarke and AE Hughes"
output: bookdown::html_document2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  fig.height = 3,
  fig.align = "center")
```

# Intro

This file contains the *registered analysis* for Hughes et al. (2023). This file requires the following:

`1_pre_process_data.R` : this script imports the data and applies our registered exclusion criteria.

As some of the Bayesian models take a while to fit, these are pre-computed using `2_fit_models.R`. 
Similarly, please run `2b_model_comparison` to recompute the posterior model weights for H1 and H2. 

`../scripts/our_functions.R` contains a range of functions for working with the model objects.

## Setup and Data Import

```{r startup load-packages, include = FALSE}
library(tidyverse)
library(brms)
library(tidybayes)
library(patchwork)
library(latex2exp)
library(ggpmisc)
library(knitr)
library(kableExtra)
```

```{r}
# set ggplot2 theme
theme_set(ggthemes::theme_tufte())

colourPalette <- c("#e78429", "#ed968c", "#7c4b73","#aadce0", "#72bcd5", "528fad") # this matches experimental stuff
colourPalette2 <- c("#648FFF", "#785EF0", "#DC267F","#FE6100", "#FFB000", "#000000") # this doesn't (colour blind friendly)

# use parallel cores for mcmc chains!
options(mc.cores = 4)

# reduce the number of decimal places
options(digits = 3)

# functions used for our Bayesian re-analysis
source("../scripts/our_functions.R")

# set seed to make sure everything is reproducible 
set.seed(100320021)
```

## Import and Remove Outliers 

We will import data from the pilot experiment (with pre-processing already in place, e.g. incorrect trials removed, 1st and 100th percentile reaction times removed). $d1$ is the training data set (Experiment 1) and $d2$ is the test data set (Experiment 2).

```{r import-data, message = FALSE, warning = FALSE}
source("1_pre_process_data.R")

summary(d1)
summary(d2)
```

# Fit Models to Data

For hypothesis 1, we fit the following models:

* A shifted lognormal model to the training data
* A lognormal model to the training data
* A normal model to the training data

For hypothesis 2, we fit the following models:

* A shifted lognormal model to the training data, with a linear number of distractors

For hypotheses 3 and 4, we fit the following models:

* A shifted lognormal model to the test data
* A shifted lognormal model to the test data with person as a fixed effect, as we want to predict person-level effects

The models take a little while to run, so do not run by default in this document. Set $eval=TRUE$ to run them.

Note that our approach differs slightly from the original methods proposed by Buetti et al (2019). In their manuscript, due to the between-subjects nature of the design, $a$ was calculated at the level of each sub-experiment. However, in our within-subjects version, we will use $a$ from the single feature experiments (i.e. Experiment 1) to predict $a$ in the compound feature experiments (i.e. Experiment 2).


```{r fit-models, eval = FALSE}
#source("2_fit_models.R")
```

## Plotting prior predictions

We can see that the priors chosen for our models centre the estimates of $D$ around 0 (i.e. we are not making any assumptions about what we expect the slopes to be).

```{r plot-priors, message = FALSE, warning = FALSE, fig.cap = "Prior predictions for our models."}

m1b <- readRDS("exp1_prior.model")

samples1b <- get_slopes(m1b, n = 1000)

samples1b %>% mutate(feature_type = if_else(
  feature %in% c("purple", "orange", "pink"), "colour", "shape")) %>%
  ggplot(aes(D, fill = feature)) + 
  geom_density(alpha = 0.33) + 
  facet_wrap(~feature_type) +
  scale_fill_manual(values=colourPalette, limits = c("orange", "pink","purple", "circle", "diamond", "triangle"))

```


## Hypothesis 1 

Test whether using a shifted-lognormal distribution gives a better model than a lognormal or normal.

```{r hypothesis-1, cache=TRUE}
m_sft <- readRDS("exp1.model")
m_nrl <- readRDS("exp1_normal.model")
m_log <- readRDS("exp1_lognormal.model")

# read in table and kable
hyp_1_table <- read_csv('model_comp_hyp1.csv')
knitr::kable(hyp_1_table) %>%
    kable_styling() 
  

```

The shifted lognormal model gives the best prediction 100% of the time, meaning that we have strong evidence that we should prefer this model over the others.

Some plots to illustrate our models:

```{r hypothesis-1-plots, message = FALSE, warning = FALSE, echo = FALSE, fig.cap = "Model predictions for (top) shifted log normal model, (middle) log normal model and (bottom) normal model.", fig.height = 10}
d1 %>% group_by(feature, lnd, observer) %>%
  summarise(median_rt = median(rt)) %>%
  summarise(mean_median = median(median_rt)) -> d_mean
  
d1_sft <- plot_model_pred(m_sft, d1)
d1_log <- plot_model_pred(m_log, d1)
d1_nrl <- plot_model_pred(m_nrl, d1)

d1_sft / d1_log / d1_nrl

rm(m_nrl, m_log)
```

```{r hyp1-plot-for-paper, include = FALSE}

# Just want the sft model plot for the main manuscript

d1 %>% modelr::data_grid(feature, lnd= seq(-1, 4, length.out = 25)) %>%
  add_predicted_draws(m_sft, re_formula = NA) %>%
  mean_hdci(.width = c(0.53, 0.97)) %>%
  ggplot(aes(lnd)) +
  geom_jitter(data = d1%>% sample_frac(0.1), aes(y = rt), colour = "#DC267F", alpha = 0.1) + 
  geom_ribbon(aes(ymin = .lower, ymax = .upper, group = .width), alpha = 0.3, fill = "#785EF0") + 
  coord_cartesian(xlim = c(-0.5, 3.75), ylim = c(0.4, 2)) + 
  facet_wrap(~feature, nrow = 1) +
  scale_x_continuous("log(number of distractors)")-> d1_sft_paper

ggsave("../../plots/sft_ln_model.pdf", d1_sft_paper, width = 8, height = 2)

```

## Hypothesis 2

Here, we want to verify that using log(number of distractors) in our model is superior to n(distractors).

```{r hypothesis-2, cache=TRUE, echo = FALSE}

hyp_2_table <- read_csv('model_comp_hyp2.csv')
knitr::kable(hyp_2_table) %>%
    kable_styling() 

```

Our loglinear model is preferred 100% of the time, supporting the hypothesis that this is the best model structure for the data.

# Hypothesis 3

## Fixed effect of D (group average)

Can we predict $D$ in the double-feature case from data in the single-feature case?
Which method has the lowest mean absolute prediction error?

```{r hypothesis-3-single, message = FALSE, warning = FALSE, fig.cap = "Predicting the group average D."}

m1 <- readRDS("exp1.model")

samples1 <- get_slopes(m1)

samples1 %>% group_by(feature) %>% 
  median_hdci(D) %>%
  select(feature, D, .lower, .upper) %>%
  knitr::kable()  %>%
  kable_styling() 

samples1 %>% mutate(feature_type = if_else(
  feature %in% c("purple", "orange", "pink"), "colour", "shape")) %>%
  ggplot(aes(D, fill = feature)) +
  geom_density(alpha = 0.33) +
  facet_grid(~feature_type) +
  scale_fill_manual(values=colourPalette,  limits = c("orange", "pink","purple", "circle", "diamond", "triangle")) -> pltDi

pltDi

```

```{r hypothesis-3-double, message = FALSE, warning = FALSE, fig.cap = "Predicting person-level effects."}

m2 <- readRDS("exp2_random.model")
samples2 <- get_slopes(m2)

#colourPalette2 <- c("#e78429", "#ed968c", "#7c4b73")

samples2 %>% separate(feature, into = c("colour", "shape")) %>%
  ggplot(aes(D, fill = colour)) +
  geom_density(alpha = 0.33) +
  facet_grid(.~shape) +
  scale_fill_manual(values=colourPalette) + 
  xlab(expression(D[c,s])) -> pltDcs

pltDcs

```

```{r hypothesis-3-plot-for-paper, include = FALSE}

ggsave("../../plots/Dcs.pdf", pltDi + pltDcs + plot_layout(widths = c(3,4)), width = 8, height = 2)


```

```{r hypothesis-3-plot-slopes, message = FALSE, warning = FALSE, fig.width = 10, fig.cap = "Predicted D plotted against empirical D for our pilot data."}
things_to_calc <- samples2 %>% select( -D, -.draw) %>% 
  distinct() %>% 
  separate(feature, c("feature1", "feature2")) 

slopes <- pmap_df(things_to_calc, calc_D) %>% 
  full_join(samples2, by = c(".draw", "feature")) %>% 
  pivot_longer(c(collinear, `best feature`, `orthogonal contrast`), names_to = "method", values_to = "Dp") %>% 
  select(-feature) %>% 
  pivot_longer(c(D, Dp), names_to = "type", values_to = "D") %>% 
  group_by(feature1, feature2, method, type) 

slopes %>% 
  median_hdci(D) %>% 
  unite(D, D, .lower, .upper) %>% 
  select(-.width, -.point, -.interval) %>% 
  pivot_wider(names_from = "type", values_from = "D") %>% 
  separate(D, into = c("De", "De_min", "De_max"), sep = "_", convert = TRUE) %>% 
  separate(Dp, into = c("Dp", "Dp_min", "Dp_max"), sep = "_", convert=  TRUE) %>% 
  ggplot(aes(x = Dp, xmin = Dp_min, xmax = Dp_max, y = De, ymin = De_min, ymax = De_max)) + 
  geom_point() + 
  geom_errorbar(alpha = 0.5) + 
  geom_errorbarh(alpha = 0.5) + 
  geom_abline(linetype = 2) + 
  geom_smooth(method = "lm", fullrange  = T, colour = "#785EF0") + 
      stat_poly_eq(formula = y ~ x, 
               aes(label = paste(..eq.label.., ..rr.label.., sep = "*plain(\",\")~")), 
               parse = TRUE, size = 2.5, label.y = 0.99, coef.digits = 3, rr.digits = 4) + 
  facet_wrap(~method, scales = "free") 

ggsave("../../plots/Dpe.pdf", width = 7, height = 3)

``` 

Calculate mean absolute prediction error: 

```{r mean-absolute-prediction-error, message = FALSE, warning = FALSE} 
slopes  %>% 
  ungroup() %>% 
  pivot_wider(names_from = "type", values_from = "D") %>% 
  mutate(abs_err = abs(D-Dp)) %>% 
  group_by(feature1, feature2, method) %>% 
  summarise(mean_abs_err = mean(abs_err)) %>% 
  pivot_wider(names_from = "method", values_from = "mean_abs_err") %>% 
  ungroup() -> slopes_err 


add_row(slopes_err, feature1 = "sum", feature2 = "over all", 
        `best feature` = sum(slopes_err[3]), 
        collinear = sum(slopes_err[4]), 
        `orthogonal contrast` = sum(slopes_err[5])) %>% 
  knitr::kable() %>%
  kable_styling() %>%
  row_spec(10,bold=T,hline_after = T)
``` 

Overall, the orthogonal model has the lowest mean absolute error.

# Hypothesis 4: Predicting Reaction Times 

## Original (Registered) Method

Finally, we check which model offers the best prediction of reaction times. 

```{r predict_rt, message = FALSE, warning = FALSE, echo = FALSE} 
compute_rt_predictions <- function(slopes, meth) {
   
   slopes %>% 
     pivot_wider(names_from = type, values_from = D) %>% 
     filter(method == meth) %>% 
     group_by(feature1, feature2) %>% 
     summarise(mu = mean(Dp), sigma = sd(Dp), .groups = "drop") %>% 
     mutate( 
       d_feature = as_factor(paste(feature1, feature2)), 
       method = meth) -> Dp_summary 

   # now define and run new model!  

 my_f <- brms::bf(rt ~ feature:lnd + (feature:lnd|observer),  
            ndt ~ 1 + (1|observer)) 
 my_inits <- list(list(Intercept_ndt = -10)    ) 

 intercept_mu <- fixef(m1)["Intercept", 1] 
 intercept_sd <- fixef(m1)["Intercept", 2] 

 ndt_int_mu <- fixef(m1)["ndt_Intercept", 1] 
ndt_int_sd <- fixef(m1)["ndt_Intercept", 2] 

   sigma_mean <-  VarCorr(m1)$residual$sd[1] 
   sigma_sd   <-  VarCorr(m1)$residual$sd[2] 

   sd_mean <- VarCorr(m1)$observer$sd[1,1] 
   sd_sd <- VarCorr(m1)$observer$sd[1,2] 

   sd_ndt_mean <- VarCorr(m1)$observer$sd[2,1] 
   sd_ndt_sd <- VarCorr(m1)$observer$sd[2,2] 

   slopes <-paste0("feature", Dp_summary$feature1, "_", Dp_summary$feature2, ":lnd") 
   slopes_mu <-Dp_summary$mu  
   slopes_sd <-Dp_summary$sigma  

    my_prior <-  c( 
     prior_string(paste("normal(", intercept_mu, ",",  intercept_sd, ")", sep = ""), class = "Intercept"), 
     prior_string(paste("normal(", slopes_mu, ",",  slopes_sd, ")", sep = ""), class = "b", coef = slopes), 
     prior_string(paste("normal(", sigma_mean, ",", sigma_sd, ")", sep = ""), class = "sigma"), 
     prior_string(paste("normal(", sd_mean, ",", sd_sd, ")", sep = ""), class = "sd"), 
     prior_string(paste("normal(",ndt_int_mu, ", ", ndt_int_sd, ")"), class = "Intercept", dpar = "ndt" ), 
     prior_string(paste("normal(",sd_ndt_mean,",", sd_ndt_sd,")"), class = "sd", dpar = "ndt")) 

     stanvars <- stanvar(sigma_mean, name='sigma_mean') +  
     stanvar(sigma_sd, name='sigma_sd') +  
     stanvar(sd_mean, name='sd_mean') +  
     stanvar(sd_sd, name='sd_sd') 

   m_prt <- brm( 
     my_f,  
     data = d2, 
     family = brmsfamily("shifted_lognormal"), 
     prior = my_prior, 
     chains = 1, 
     iter = 80000, 
     inits = my_inits, 
     stanvars = stanvars, 
     save_pars = save_pars(all=TRUE), 
     silent = TRUE, 
     sample_prior = "only", 
     refresh = 0) 

   return(m_prt) 
 } 


``` 

```{r hypothesis-4-plot, cache=TRUE, message = FALSE, warning = FALSE, echo = FALSE, fig.height = 5, , fig.cap = "Predicted RTs for our data, using orthogonal contrast combination.", cache.lazy = FALSE} 

d2 <- d2 %>% unite(feature, feature1, feature2) # is this line of code redundant?

m_col <- compute_rt_predictions(slopes, "collinear") 
m_bfe <- compute_rt_predictions(slopes, "best feature") 
m_orc <- compute_rt_predictions(slopes, "orthogonal contrast") 

 d2 %>% modelr::data_grid(feature, lnd) %>% 
   add_predicted_draws(m_orc, re_formula = NA) %>% 
   mean_hdci(.width = c(0.53, 0.97)) %>% 
   ggplot(aes(lnd)) + 
   geom_ribbon(aes(ymin = .lower, ymax = .upper, group = .width), alpha = 0.3, fill = "#785EF0") +  
   geom_jitter(data = d2, aes(y = rt), colour = "#DC267F", alpha = 0.01) +  
   facet_wrap(~feature, nrow = 3) 


``` 


```{r hypothesis-4-bridge-sample, echo = FALSE, message = FALSE, warning = FALSE, cache=TRUE} 
 m_colbs <- bridge_sampler(m_col, silent = TRUE) 
 m_bfebs <- bridge_sampler(m_bfe, silent = TRUE) 
 m_orcbs <- bridge_sampler(m_orc, silent = TRUE) 

 tibble(model = c("collinear", "best feature", "orthogonal contrast"), 
              `posterior probability` = post_prob(m_colbs, m_bfebs, m_orcbs)) %>% 
   knitr::kable() %>%
   kable_styling()

``` 

All three models receive approximately the same predictive weight, meaning that we are not able to distinguish between them: we do not have evidence for one model over the others.  

## New, Improved (Unregistered) Method

For reasons explained in the manuscript...


First, read in data and models again: 
```{r}
source("pred_rt_functions.R")
source("1_pre_process_data.R")

d1 %>% mutate(observer = as.numeric(observer)) %>%
  select(-nd) -> d1

d2 %>% mutate(observer = as.numeric(observer)) %>%
  select(-nd) -> d2

d2 %>% unite(feature, feature1, feature2) -> d2

m1 <- readRDS("exp1.model") 
m2 <- readRDS("exp2_random.model") 
```

now merge

```{r get_rt_models, cache=TRUE}
ndraws <- 100

model1 <- get_rt_model_simple(m1)
rm(m1)
model2 <- get_rt_model_simple(m2)
rm(m2)

# convert in order to get D predictions for m2
full_join(
  mod_col <- model1 %>% filter(feature %in% c("orange", "pink", "purple")) %>%
    rename(feature1 = "feature", D1 = "D"),
  mod_sha <- model1 %>% filter(feature %in% c("circle", "diamond", "triangle")) %>%
    rename(feature2 = "feature", D2 = "D")) %>%
  unite(feature, feature1, feature2) %>%
  mutate(D_collinear = 1/((1/D1) + (1/D2)),
         D_best_feature = pmin(D1, D2),
         D_orth_contrast =  1/sqrt(1/(D1^2) + (1/D2^2))) %>%
  select(.draw,  feature, D_collinear, D_best_feature, D_orth_contrast) -> Dp

full_join(model2, Dp) %>%
  pivot_longer(c(D, D_collinear, D_best_feature, D_orth_contrast), 
               names_to = "method", values_to = "b") -> models

rm(Dp)
rm(model1, model2)


make_pred <- function(drw) {
  
  draw  <- unique(models$.draw)[drw]
  
  mod <- filter(models, .draw == draw)
  
  full_join(mod, 
            d2 %>% filter(lnd>0) , 
            by = c("feature")) %>%
    select(.draw, method, feature, lnd, a, b, ndt, sd, rt) %>%
    mutate(p_mu_rt = exp(ndt) + exp(a + b*lnd),
           loglik = dshifted_lnorm(rt, meanlog = log(p_mu_rt), sdlog = sd, shift = ndt, log = T)) %>%
    arrange(method, feature, lnd) -> dp
  
  return(dp)

}

dp <- map_df(1:ndraws, make_pred)
```

### Absolute Error

```{r}

dp %>% 
  select(.draw, method, feature, lnd, rt, p_mu_rt, loglik) %>%
  mutate(abs_err = abs(rt-p_mu_rt)) -> dp

dp %>%  group_by(.draw, method, lnd) %>%
  summarise(sum_err = sum(abs_err)) %>%
  pivot_wider(names_from = "method", values_from = "sum_err") %>%
  pivot_longer(c(D_collinear, D_best_feature, D_orth_contrast), 
               names_to = "method", values_to = "Dp") %>%
  mutate(rel_sum_abs_err = Dp/D) -> Derr

Derr %>% group_by(method) %>% 
  median_hdi(rel_sum_abs_err, .width = 0.97) %>%
  knitr::kable() %>%
  kable_styling() 

Derr %>% ggplot(aes(lnd, rel_sum_abs_err, fill = method)) +
  stat_lineribbon(alpha = 0.2, .width = 0.97) +
  facet_wrap(~method) +
  geom_hline(yintercept = 1, linetype = 2) + 
  ggtitle("all 3 methods do as well as De")

```
