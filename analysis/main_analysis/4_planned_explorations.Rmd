---
title: "4 Planned Explorations"
author: "Alasdair D.F. Clarke and Anna E. Hughes"
date: "2023-03-24"
output: bookdown::html_document2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  fig.height = 3,
  fig.align = "center")
```

# Introduction

```{r load-packages, include = FALSE}
library(tidyverse)
library(brms)
library(tidybayes)
library(patchwork)
library(latex2exp)
library(ggpmisc)
library(ggridges)
library(corrr)
library(knitr)
library(kableExtra)
```

```{r}
# set ggplot2 theme
theme_set(ggthemes::theme_tufte())
colourPalette <- c("#e78429", "#ed968c", "#7c4b73","#aadce0", "#72bcd5", "528fad")

# use parallel cores for mcmc chains!
options(mc.cores = 4)

# reduce the number of decimal places
options(digits = 3)

# functions used for our Bayesian re-analysis
source("../scripts/our_functions.R")

# set seed to make sure everything is reproducible 
set.seed(100320021)
```

# Individual Differences in D

First, we will look to see the extent to which $D$ varies from one person to the next. We will also ask whether these variations are correlated, both for single and for double feature search. 


```{r cache=TRUE, warning = FALSE}
## Import and Remove Outliers 

source("1_pre_process_data.R")

m1 <- readRDS("exp1.model")
m2 <- readRDS("exp2_random.model")

# Calculate slopes, fixed=FALSE means we will return samples per-observer, rather
# than simply the fixed effects. 
samples1 <- get_slopes(m1, fixed = F)
samples2 <- get_slopes(m2, fixed = F) 
```

## Correlations for single feature search

```{r create_D_obs_plot, cache=TRUE}
ggplot(samples1, aes(x=D, y= as_factor(feature), 
                     group = interaction(observer, feature), fill = feature)) +
  geom_density_ridges(alpha = 0.5) +
  scale_fill_manual(values = colourPalette) + 
  scale_x_continuous(expression(D[c] ~ and ~ D[s])) + 
  scale_y_discrete("feature") + 
  theme(legend.position = "none") -> plt_D_obs
```

Compute range of fixed effect (shallowest to steepest fixed effect)

```{r}
 get_slopes(m1, fixed = T) %>%
  group_by(feature) %>%
  summarise(D = mean(D)) -> mean_slopes

max(mean_slopes$D) - min(mean_slopes$D)
  
```
Now look within a feature

```{r}
samples1 %>% group_by(feature, observer) %>%
  summarise(D = mean(D)) %>%
  summarise(minD = min(D), 
            maxD = max(D),
            range = maxD - minD)
```



Define a little function for computing correlation for a sample from our posterior:

```{r}
compute_corr <- function(drw, df) {
  df %>% filter(.draw==drw) %>%
    select(-.draw) %>%
    correlate(quiet = TRUE) %>%
    stretch() -> dout
  
  return(dout)
}
```

Apply this function to all the samples in our posterior: 

```{r compute_single_feature_corr, cache=TRUE}
samples1 %>% 
  pivot_wider(names_from = "feature", values_from = "D") %>%
  select(-observer) -> dat_for_corr

dcorr <- map_df(unique(dat_for_corr$.draw), compute_corr, dat_for_corr) %>%
  mutate(x = as_factor(x), 
         x = fct_relevel(x, "pink", "purple", "orange"), 
         y = as_factor(y),
         y = fct_relevel(y, "pink", "purple", "orange"))
```

```{r create_single_feat_plot, cache=TRUE}
dcorr %>% group_by(x, y) %>%
  median_hdci(r, .width = 0.97) %>%
  mutate(fill = if_else(.lower>0, "nonezero", "zero")) %>%
  select(x, y, fill) -> dfill


ggplot(left_join(dcorr, dfill, by = c("x", "y")), aes(x, r)) +
  stat_slabinterval(alpha = 0.5, aes(fill = fill), 
                    point_interval = "median_hdci",
                    .width = c(0.53, 0.97), fatten_point = 0) + 
  geom_hline(yintercept = 0, linetype = 2) + 
  facet_wrap(~y) +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust=1),
        legend.position = "none",
        axis.title.x = element_blank()) +
  scale_y_continuous("Pearson's correlation coefficent", 
                     limits = c(-0.6, 1), expand = c(0, 0), breaks = seq(-0.5, 1, 0.5)) +
  scale_fill_manual(values = c("darkgreen", "black")) -> plt_corr
```

```{r plot-single-feature-corr, fig.cap = "Indiviual differences in Dc and Ds. (left) Posterior probability distributions for Dc and Ds for each individual. (right) Estimated correlations between each of the Dc and Ds.", warning = FALSE, message = FALSE, echo = FALSE}

plt_D_obs + plt_corr

```

```{r, include = FALSE}

ggsave("../../plots/single_feature_correlations.pdf", height = 2.5, width = 7)

rm(plt_D_obs, plt_corr)
```

## Correlations for double feature search 

```{r compute_double_feature_corr, cache=TRUE}
######### Now do double feature search
samples2 %>% 
  pivot_wider(names_from = "feature", values_from = "D") %>%
  select(-observer) -> dat_for_corr

dcorr <- map_df(unique(dat_for_corr$.draw), compute_corr, dat_for_corr) %>%
  mutate(x = as_factor(x), 
         y = as_factor(y))
```

```{r plot-cor-double-feature, fig.height=6, fig.cap = "Estimated correlations for double feature search.", warning = FALSE, echo = FALSE}
ggplot(dcorr, aes(x, r)) +
  stat_slabinterval(.width = c(0.53, 0.97)) + 
  geom_hline(yintercept = 0, linetype = 2) + 
  facet_wrap(~y) +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust=1))

rm(dcorr)
```

## Predicting $D$ per person

In the main manuscript, we combine the single feature search slopes to predict the double feature conditions across all participants. Here, we show the predictions for each participant individually.

```{r calc_D, cache=TRUE}
#########
# Look at how well we can predict Dcs per person
calc_D <- function(feature1, feature2, observer) {
  
  obs <- observer
  D1 <- filter(samples1, feature == feature1, observer == obs)$D
  D2 <- filter(samples1, feature == feature2, observer == obs)$D
  
  # now calculate D_overall using the three proposed methods
  D_collinear = 1/((1/D1) + (1/D2))
  D_best_feature = pmin(D1, D2)
  D_orth_contrast =  1/sqrt(1/(D1^2) + (1/D2^2))
  
  return(tibble(.draw = 1:length(unique(samples1$.draw)),
                observer = obs,
                feature = paste(feature1, feature2, sep = "_"),
                feature1 = feature1, feature2 = feature2,
                collinear = D_collinear,
                `best feature` = D_best_feature,
               `orthogonal contrast` = D_orth_contrast,
               ))
}
```

```{r running_calcD, cache=TRUE}
things_to_calc <- samples2 %>% 
  select(-.draw, -D) %>%
  distinct() %>% 
  separate(feature, c("feature1", "feature2")) %>%
  select(feature1, feature2, observer)

slopes <- pmap_df(things_to_calc, calc_D) %>% 
  full_join(samples2, by = c("observer", ".draw", "feature")) %>%
  pivot_longer(c(collinear, `best feature`, `orthogonal contrast`), names_to = "method", values_to = "Dp") %>% 
  group_by(observer, method, feature ) %>%
  summarise(De = median(D), 
            Dp = median(Dp), .groups = "drop")
```

```{r plot-Dp, fig.height = 15, fig.width = 10, fig.cap = "D predictions for each observer.", message = FALSE}
ggplot(slopes, aes(Dp, De, colour = method)) + geom_point() +
  geom_smooth(method = lm, alpha = 0.25) +
  geom_abline(linetype = 2) + 
  facet_wrap(~observer, nrow = 8)
```

```{r comp_corr, cache=TRUE}
comp_corr <- function(obs, meth) {
  df <- filter(slopes, observer == obs, method == meth)
  r <- cor(df$De, df$Dp)
  return(r)
}

r_col <- map_dbl(1:40, comp_corr, "collinear")
r_bfe <- map_dbl(1:40, comp_corr, "best feature")
r_oth <- map_dbl(1:40, comp_corr, "orthogonal contrast")
```

Below, we can see that across all participants, orthogonal contrast generally has the highest correlation coefficient value, although there are several participants who are very poorly predicted by the orthogonal contrast method (unlike the collinear and the best feature methods).

TO DO: comp_corr with absolue error (rather than r2 value).

```{r plot-Dmeth-corr-hist, fig.cap = "Histogram of correlation coefficient values for each combination method.", message = FALSE, warning = FALSE}
tibble(method = rep(c("collinear", "best feature", "orthogonal contrast"), 
                    each=40),
                r = c(r_col, r_bfe, r_oth)) %>%
  ggplot(aes(r, fill = method)) + geom_histogram(alpha = 0.5, position=position_identity())

```


```{r, include = FALSE}
rm(list = ls(all.names = TRUE))
```


# Ring Things

 First, read in data again:
 
```{r}
source("1_pre_process_data.R")
```
 

## Model comparison 

First, we will compare the ring model to a non-ring model (for the training dataset).

```{r ring_mod_comp}

### first compare ring model to non-ring
m1 <- readRDS("exp1_ring.model")
m0 <- readRDS("exp1.model")

# read in csv and kable
ring_table <- read_csv('model_comp_ring.csv', show_col_types = FALSE)
knitr::kable(ring_table) 

```

We can see that the ring model is preferred to the non-ring model - it offers a better model of the data.

```{r include = FALSE}

rm(m0)

```

Does ring interact with the distractor type?

```{r plot-ring-model, cache=TRUE, fig.cap = "Posterior predictions for the ring model."}

m1 %>% gather_draws(`b_.*`, regex=T) %>%
  select(-.chain, -.iteration) %>% 
  filter(!str_detect(.variable, "ndt")) %>%
  mutate(.variable = str_remove(.variable, "b_"),
         lin_mod_comp = if_else(str_detect(.variable, ":"), "slope", "intercept"),
         ring = as_factor(parse_number(.variable))) -> post

post %>% filter(lin_mod_comp == "slope") %>%
  mutate(feature = str_extract(.variable, "orange|pink|purple|diamond|circle|triangle")) %>%
  ungroup() %>%
  select(-.variable, -lin_mod_comp) -> slopes
  
slopes %>% ggplot(aes(.value, fill = ring)) + geom_density(alpha = 0.5) +
  facet_wrap(~feature) +
  ggthemes::scale_fill_ptol()

```


``` {r plot-ring-model-2, fig.cap = "Fixed effects for predicting the effect of ring, feature and number of distractors on response times."}

d1 %>% modelr::data_grid(feature, ring, lnd = seq(0, 3.5, 0.1)) %>%
  add_epred_draws(m1, ndraws = 100, re_formula = NA) -> pred

ggplot(pred, aes(x = lnd, y = .epred, colour = ring, group = .draw)) +
  geom_path(alpha = 0.1) + 
  facet_wrap(~feature) +
  ggthemes::scale_color_ptol()

```

``` {r include = FALSE}

ggsave("../../plots/ring_single_feature.pdf", width = 7, height = 3)

```

As might be expected, the answer seems to be yes, the effect of eccentricity does seem to depend on the exact distractor type - in particular, the slopes seem to be steeper for the distractors more similar in shape to the target for the furthest eccentricity. Conversely, in some displays (e.g. where the colour is more different) there seems to be little effect of eccentricity.

## Predicting D

```{r cache=TRUE}
# now read in m2 so that we can compute Dp
m2 <- readRDS("exp2_ring.model")

# get slopes!
samples1 <- get_slopes(m1, rings = TRUE)
samples2 <- get_slopes(m2, num_features = 2, rings = TRUE)  
```

```{r}
calc_D <- function(ring, feature1, feature2) {
  
  rn = ring
  D1 <- filter(samples1, ring == rn, feature == feature1)$D
  D2 <- filter(samples1, ring == rn, feature == feature2)$D
  
  # now calculate D_overall using the three proposed methods
  D_collinear = 1/((1/D1) + (1/D2))
  D_best_feature = pmin(D1, D2)
  D_orth_contrast =  1/sqrt(1/(D1^2) + (1/D2^2))
  
  return(tibble(.draw = 1:length(unique(samples1$.draw)),
                ring = ring,
               # feature = paste(feature1, feature2, sep = "_"),
                feature1 = feature1, 
               feature2 = feature2,
                collinear = D_collinear,
                `best feature` = D_best_feature,
                `orthogonal contrast` = D_orth_contrast))
}

```

```{r calc-D-run, cache=TRUE, message = FALSE, fig.cap = "The relationship between predicted D and empirical D for the three methods, shown for each ring separately."}

things_to_calc <- samples2 %>% select( -D, -.draw) %>% 
   distinct()

slopes <- pmap_df(things_to_calc, calc_D) %>% 
  full_join(samples2, 
            by = c(".draw", "feature1", "feature2", "ring")) %>% 
  pivot_longer(c(collinear, `best feature`, `orthogonal contrast`), names_to = "method", values_to = "Dp") %>% 
  pivot_longer(c(D, Dp), names_to = "type", values_to = "D") %>% 
  group_by(feature1, feature2, ring,  method, type) 

slopes %>% 
  median_hdci(D) %>%
  unite(D, D, .lower, .upper) %>% 
  select(-.width, -.point, -.interval) %>% 
  pivot_wider(names_from = "type", values_from = "D") %>% 
  separate(D, into = c("De", "De_min", "De_max"), sep = "_", convert = TRUE) %>% 
  separate(Dp, into = c("Dp", "Dp_min", "Dp_max"), sep = "_", convert=  TRUE) %>%
  filter(is.finite(ring)) %>% 
  ggplot(aes(x = Dp, xmin = Dp_min, xmax = Dp_max, y = De, ymin = De_min, ymax = De_max, colour = as_factor(ring))) + 
  geom_point() + 
  geom_errorbar(alpha = 0.5) + 
  geom_errorbarh(alpha = 0.5) + 
  geom_abline(linetype = 2) + 
  geom_smooth(method = "lm", fullrange  = T) +
  # stat_poly_eq(formula = y ~ x, 
  #              aes(label = paste(..eq.label.., ..rr.label.., sep = "*plain(\",\")~")), 
  #              parse = TRUE, size = 2.8, label.y = 0.9, coef.digits = 3, rr.digits = 4, colour="blue") + 
  facet_wrap(~method, scales = "free") +
  ggthemes::scale_color_ptol("target ring")

```

``` {r include = FALSE}

ggsave("../../plots/target_ring_D_pred.pdf",  width = 8, height = 3)

```


```{r}

# compute prediction error per feature per ring

slopes  %>% 
  ungroup() %>% 
  pivot_wider(names_from = "type", values_from = "D") %>% 
  mutate(abs_err = abs(D-Dp)) %>% 
  group_by(ring, feature1, feature2, method) %>% 
  summarise(mean_abs_err = mean(abs_err), .groups = "drop") %>% 
  pivot_wider(names_from = "method", values_from = "mean_abs_err") %>% 
  ungroup() -> slopes_err 


add_row(slopes_err, ring = 0, feature1 = "mean", feature2 = "over all", 
        `best feature` = mean(slopes_err[4][[1]]), 
        collinear = mean(slopes_err[5][[1]]), 
        `orthogonal contrast` = mean(slopes_err[6][[1]])) %>% 
  knitr::kable() %>%
  kable_styling() %>%
  row_spec(28,bold=T,hline_after = T)


```

```{r}
## compute regression slopes for Dp to De, ignoring ring and feature
slopes %>% pivot_wider(names_from = "type", values_from = "D") -> Dlm

summary(lm(data = Dlm, D ~ 0 + Dp:method))

```

## Tidy up

```{r}
rm(post, pred, ring_table, samples1, samples2, slopes, things_to_calc)
```



# Predicting Reaction Times

First, read in data and models again: 
```{r}
source("pred_rt_functions.R")
source("1_pre_process_data.R")

d1 %>% mutate(observer = as.numeric(observer)) %>%
  select(-nd) -> d1

d2 %>% mutate(observer = as.numeric(observer)) %>%
  select(-nd) -> d2

d2 %>% unite(feature, feature1, feature2) -> d2

m1 <- readRDS("exp1_ring.model") 

m2 <- readRDS("exp2_ring.model") 
```

now merge

```{r get_rt_models, cache=TRUE}
ndraws <- 100

model1 <- get_rt_model_ring_obs(m1)
rm(m1)
model2 <- get_rt_model_ring_obs(m2)
rm(m2)

# convert in order to get D predictions for m2
full_join(
  model1 %>% filter(feature %in% c("orange", "pink", "purple")) %>%
    rename(feature1 = "feature", D1 = "D"),
  model1 %>% filter(feature %in% c("circle", "diamond", "triangle")) %>%
    rename(feature2 = "feature", D2 = "D"),
  by = c(".draw", "observer", "ring", "a", "ndt", "sd")) %>%
  unite(feature, feature1, feature2) %>%
  mutate(D_collinear = 1/((1/D1) + (1/D2)),
         D_best_feature = pmin(D1, D2),
         D_orth_contrast =  1/sqrt(1/(D1^2) + (1/D2^2))) %>%
  select(.draw, observer, ring, feature, D_collinear, D_best_feature, D_orth_contrast) -> Dp

# merge everything
full_join(model2, Dp, 
          by = c(".draw", "observer", "ring", "feature")) %>%
  pivot_longer(c(D, 
                 D_collinear, D_best_feature, D_orth_contrast), 
               names_to = "method", values_to = "b") -> models

rm(Dp)
rm(model1, model2)

make_pred <- function(drw) {
  
  draw  <- unique(models$.draw)[drw]
  
  mod <- filter(models, .draw == draw)
  
  full_join(mod, d2, by = c("observer", "ring", "feature")) %>%
    select(.draw, method, observer, ring, feature, lnd, a, b, ndt, sd, rt) %>%
    mutate(p_mu_rt = exp(ndt) + exp(a + b*lnd),
           loglik = dshifted_lnorm(rt, meanlog = log(p_mu_rt), sdlog = sd, shift = ndt, log = T),
           abs_err = abs(rt-p_mu_rt)) %>%
    arrange(method, observer, ring, feature, lnd)  %>% 
  select(.draw, method, observer, ring, feature, lnd, rt, p_mu_rt, loglik, abs_err)-> dp
  
  return(dp)

}

dp <- map_df(1:ndraws, make_pred)
```

### Absolute Error

```{r}

dp %>% 
  filter(lnd > 0) %>%
  group_by(.draw,lnd, method) %>%
  summarise(mean_err = mean(abs_err), .groups = "drop") %>%
  pivot_wider(names_from = "method", values_from = "mean_err") %>%
  pivot_longer(c(D_collinear, D_best_feature, D_orth_contrast), 
               names_to = "method", values_to = "Dp") %>%
  mutate(rel_mean_abs_err = Dp/D) -> Derr

Derr %>% ungroup() %>%
  group_by(method) %>% 
  median_hdci(rel_mean_abs_err, .width = 0.97) %>%
  knitr::kable() %>%
  kable_styling() 

Derr %>% ggplot(aes(lnd, rel_mean_abs_err, fill = method)) +
  stat_lineribbon(alpha = 0.2, .width = 0.97) +
  facet_grid(.~method) +
  geom_hline(yintercept = 1, linetype = 2) +
  ggthemes::scale_color_ptol() +
    coord_cartesian(ylim = c(0.99, 1.1))

```
